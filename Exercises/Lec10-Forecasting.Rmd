---
title: "Lecture 10 - Forecasting and Fitting ARIMA Models"
author: "Ethan Shen"
date: "5/17/2020"
output: 
  html_document:
    keep_md: TRUE
---

```{r setup, include=FALSE}
library(tidyverse)
ggplot2::theme_set(ggplot2::theme_bw())
```

# AR(1) Example

$\phi = 0.75$, $\delta=0.5$, and $\sigma_w^2=1$,

```{r message=FALSE, warning=FALSE}
ar1 = arima.sim(n=1000, model = list(order=c(1,0,0), ar=0.75), mean=0.5)
forecast::ggtsdisplay(ar1)
```

### Using ARIMA function

```{r}
ar1_arima = forecast::Arima(ar1, order = c(1,0,0)) 
summary(ar1_arima)
```

The "mean" reported by the `ARIMA` model is $E(y_t) = \frac{\delta}{1-\phi}$.

### Using lm() function

```{r message=FALSE, warning=FALSE}
d = data_frame(y = ar1, t=seq_along(ar1))
ar1_lm = lm(y~lag(as.vector(y)), data=d)
summary(ar1_lm)
```

We see the coefficients for `ar1` and `lag(y)` are very very similar. 


# Bayesian AR(1) Model 

Model follows decomposition of multivariate normal density into product of conditional densities.

```{r}
ar1_model = "model{
# likelihood
  y[1] ~ dnorm(delta/(1-phi), (sigma2w/(1-phi^2))^-1) # taking inverse of variance bc JAGS takes precision, not variance
   
   # posterior predictive
  y_hat[1] ~ dnorm(delta/(1-phi), (sigma2w/(1-phi^2))^-1)

 # conditional likelihoods
  for (t in 2:length(y)) {
    y[t] ~ dnorm(delta + phi*y[t-1], 1/sigma2w)
    y_hat[t] ~ dnorm(delta + phi*y[t-1], 1/sigma2w)
  }
  
  # expected value of y_t
  mu = delta/(1-phi)

# priors
  delta ~ dnorm(0,1/1000)
   # could be problematic bc model might not be stationary (in this case, we know the model is stationary, so not an issue)
  # however, if we don't know whether or not model is stationary, the mean and variance of y_t is not what we coded, because those are derived after assuming process is stationary
  
  phi ~ dnorm(0,1)
  tau ~ dgamma(0.001,0.001)
  sigma2w <- 1/tau
}"

m_ar1 = rjags::jags.model(textConnection(ar1_model),
                          data = list(y=ar1))

update(m_ar1, n.iter = 1000)

samp_ar1 = rjags::coda.samples(m_ar1, n.iter = 5000,
                               variable.names = c("delta", "phi", "sigma2w", "mu", "y_hat"))
```

### MCMC Diagnostics

```{r}
tidybayes::gather_draws(samp_ar1, delta, phi, sigma2w) %>%
  ggplot(aes(x = .iteration, y = .value, color = as.factor(.variable))) + 
  geom_line(alpha = 0.8) + 
  facet_wrap(~.variable, ncol=1, scales = "free_y") + 
  labs(color = "Parameter")
```

## Posterior Density & Comparison to ARIMA fit and lm() fit

```{r}
ar1_res = bind_rows(
  data_frame(
    model = "truth",
    term = c("delta", "phi", "sigma2_w"), 
    .value = c(0.5, 0.75, 1)
  ),
  data_frame(
    model = "lm",
    term = c("delta", "phi", "sigma2_w"), 
    .value = c(coef(ar1_lm), var(ar1_lm$residuals))
  ),
  data_frame(
    model = "ARIMA",
    term = c("delta", "phi", "sigma2_w"), 
    .value = c(ar1_arima$model$phi, (1-ar1_arima$model$phi)*ar1_arima$coef[2], ar1_arima$sigma2)
  )
)

tidybayes::gather_draws(samp_ar1, delta, phi, sigma2w) %>%
  ggplot(aes(x=.value)) +
  geom_density(fill="lightgrey") +
  geom_vline(data=ar1_res, aes(xintercept = .value, linetype=model, color=model), size=1.5, alpha=0.75) +
  facet_wrap(~term, ncol=3, scales = "free_x")
```

We see that $\sigma^2$ is predicted pretty well, others not so much. 

## Predictions

```{r warning=FALSE}
models = d %>% 
  modelr::add_predictions(ar1_lm,var = "lm") %>%
  mutate(
    ARIMA = ar1_arima %>% fitted(),
    bayes = tidybayes::gather_draws(samp_ar1, y_hat[i]) %>%  summarize(post_mean = mean(.value)) %>% pull(post_mean)
  ) %>%
  tidyr::gather(model, y, -t)

gridExtra::grid.arrange(
  models %>%
    filter(model=="y" | model=="lm") %>%
    ggplot(aes(x=t, y=y, color=forcats::as_factor(model))) +
    geom_line(alpha=0.75) +
    scale_color_manual(values=c("black", "red")) +
    labs(color="Model")
  ,
  models %>%
    filter(model=="y" | model=="bayes") %>%
    ggplot(aes(x=t, y=y, color=forcats::as_factor(model))) +
    geom_line(alpha=0.75) +
    scale_color_manual(values=c("black", "light blue")) +
    labs(color="Model")
  ,
  models %>%
    filter(model=="y" | model=="ARIMA") %>%
    ggplot(aes(x=t, y=y, color=forcats::as_factor(model))) +
    geom_line(alpha=0.75) +
    scale_color_manual(values=c("black", "light green")) +
    labs(color="Model")
)
```

We see the predictions (ARIMA, lm, and Bayesian) are all very similar to the truth.

# ARMA(2,2)

$\phi = (1.3,-0.5)$, $\theta = (0.5,0.2)$, $\delta=0$, and $\sigma_w^2=1$ using the same models 

```{r}
y = arima.sim(n=500, model=list(ar=c(1.3,-0.5), ma=c(0.5,0.2))) 

forecast::ggtsdisplay(y, points = FALSE)
```

PACF shows MA(2), ACF implies AR of some sort. 

## ARIMA

```{r}
forecast::Arima(y, order = c(2,0,2), include.mean = FALSE) %>% summary()
```

$\theta$ generally harder to estimate, but all estimates are relatively close to true values. 

## AR only lm

```{r}
lm(y ~ lag(as.vector(y),1) + lag(as.vector(y),2)) %>% summary()
```

These are estimates of $\phi_1$ and $\phi_2$. However, we can't do `lag(w_t)` in a simple `lm` model. 

# Hannan-Rissanen Algorithm 

## Steps 1 and 2

```{r}
ar_yw=ar.yw(y, order.max=10)
ar_yw
ar_mle=ar.mle(y, order.max=10)
ar_mle
```

## Step 3

```{r}
d = data_frame(
  y = y, 
  # create column of residuals 
  w_hat1 = ar_mle$resid 
)

# then take the lag of residuals
(lm1 = lm(y ~ lag(as.vector(y),1) + lag(as.vector(y),2) + lag(as.vector(w_hat1),1) + lag(as.vector(w_hat1),2), data=d)) %>%
  summary()
```

There is uncertainty on the residuals, so we refit by taking residuals of `lm1` and using those instead. 

## Step 4.1

```{r}
d = modelr::add_residuals(d,lm1,"w_hat2")

(lm2 = lm(y ~ lag(as.vector(y),1) + lag(as.vector(y),2) + lag(as.vector(w_hat2),1) + lag(as.vector(w_hat2),2), data=d)) %>%
  summary()
```

## Step 4.2

```{r}
d = modelr::add_residuals(d,lm2,"w_hat3")

(lm3 = lm(y ~ lag(as.vector(y),1) + lag(as.vector(y),2) + lag(as.vector(w_hat3),1) + lag(as.vector(w_hat3),2), data=d)) %>%
  summary()
```

## RMSEs

```{r}
modelr::rmse(lm1, data = d)

modelr::rmse(lm2, data = d)

modelr::rmse(lm3, data = d)
```

Once coefficients get "close" to each other, we can say they've converged. We are done. 

# Bayesian ARMA(2,2)

This model looks like $y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + w_t + \theta_1 w-{t-1} + \theta_2 w-{t-2} + e_t$, where $e_t \sim N(0, \sigma_e ^2)$. $e_t$ is NOT autoregressive, so this makes we are fitting a slightly different model compared to the `lm` model, but very similar to the Hannan Rissanen. The set of residuals that are left over (are not part of the $w_t$) are similar to the $e_t$. 

```{r}
arma22_model = "model{
# Likelihood
  for (t in 1:length(y)) {
    y[t] ~ dnorm(mu[t], 1/sigma2_e)
    y_hat[t] ~ dnorm(mu[t], 1/sigma2_e)
  }                    
  
  # estimating first two steps with latent errors 

  mu[1] = phi[1] * y_0  + phi[2] * y_n1 + w[1] + theta[1]*w_0  + theta[2]*w_n1
  mu[2] = phi[1] * y[1] + phi[2] * y_0  + w[2] + theta[1]*w[1] + theta[2]*w_0   
  for (t in 3:length(y)) { 
    mu[t] = phi[1] * y[t-1] + phi[2] * y[t-2] + w[t] + theta[1] * w[t-1] + theta[2] * w[t-2]
  }
  
# Priors

# similar to HR algorithm, we are estimating w_t
  for(t in 1:length(y)){
    w[t] ~ dnorm(0,1/sigma2_w)
  }

  sigma2_w = 1/tau_w; tau_w ~ dgamma(0.001, 0.001) 
  sigma2_e = 1/tau_e; tau_e ~ dgamma(0.001, 0.001) 
  for(i in 1:2) {
    phi[i] ~ dnorm(0,1)
    theta[i] ~ dnorm(0,1)
  }

# Latent errors and series values
  w_0  ~ dt(0,tau_w,2)
  w_n1 ~ dt(0,tau_w,2)
  y_0  ~ dnorm(0,1/1000)
  y_n1 ~ dnorm(0,1/1000)
}"
```

## Bayesian Fit

```{r}
if (!file.exists("arma22_coda.rds")) {
  m = rjags::jags.model(
    textConnection(arma22_model), 
    data = list(y = y), 
    quiet = TRUE
  ) 
  
  update(m, n.iter=50000, progress.bar="none")
  
  arma22_coda = rjags::coda.samples(
    m, variable.names=c("phi", "theta", "sigma2_w", 
                        "mu", "y", "y_hat"), 
    n.iter=50000, progress.bar="none", thin = 50
  )
  
  saveRDS(arma22_coda, "arma22_coda.rds")
} else {
  arma22_coda = readRDS("arma22_coda.rds")
}
```

```{r warning=FALSE}
arma22_res = bind_rows(
  data_frame(
    model = "True",
    term = c("phi[1]", "phi[2]", "theta[1]", "theta[2]"), 
    estimate = c(1.3, -0.5, 0.5, 0.2)
  ),
  data_frame(
    model = "Arima",
    term = c("phi[1]", "phi[2]", "theta[1]", "theta[2]"), 
    estimate = c(1.2843,  -0.5182,  0.4965,  0.2204)
  ),
  data_frame(
    model = "HR w_hat3",
    term = c("phi[1]", "phi[2]", "theta[1]", "theta[2]"), 
    estimate = c(1.23758, -0.47868, 0.54084, 0.25991)
  )
)


arma22_params = tidybayes::gather_samples(arma22_coda, phi[i], theta[i]) %>%
  ungroup() %>%
  mutate(term = paste0(term,"[",i,"]"))
```


### MCMC Diagnostics 

```{r}
arma22_params %>%
  group_by(term) %>%
  slice(seq(1,n(),n()/200)) %>%
  ggplot(aes(x=.iteration, y=estimate, color=term)) +
  geom_line() +
  facet_grid(term~., scales = "free_y")
```

These are pretty hard to sample with JAGS. 

### Posterior Densities

```{r}
arma22_params %>%
  ggplot(aes(x=estimate)) +
  geom_density(fill="lightgrey") +
  geom_vline(data=arma22_res, aes(xintercept = estimate, color=forcats::as_factor(model), 
                                  linetype=forcats::as_factor(model)), size=1, alpha=0.5) +
  labs(color="Model", linetype="Model") + 
  facet_wrap(~term, ncol=2, scales = "free")
```

All these predictions (ARIMA vs. HR) are all pretty similar to each other. 

### Predictions 

```{r warning=FALSE}
arma_models = data_frame(
  t = seq_along(y),
  ARIMA = forecast::Arima(y, order = c(2,0,2), include.mean = FALSE) %>% fitted(),
  bayes = tidybayes::gather_draws(arma22_coda, y_hat[i]) %>%
    ungroup() %>%
    mutate(.variable = paste0(.variable,"[",i,"]")) %>%
    group_by(.variable) %>%
    summarize(post_mean = mean(.value)) %>%
    pull(post_mean),
  y = y
) %>%
  tidyr::gather(model, y, -t)
gridExtra::grid.arrange(
  arma_models %>%
    filter(model=="y" | model=="bayes") %>%
    ggplot(aes(x=t, y=y, color=forcats::as_factor(model))) +
    geom_line(alpha=0.75) +
    scale_color_manual(values=c("light blue","black")) +
    labs(color="Model")
  ,
  arma_models %>%
    filter(model=="y" | model=="ARIMA") %>%
    ggplot(aes(x=t, y=y, color=forcats::as_factor(model))) +
    geom_line(alpha=0.75) +
    scale_color_manual(values=c("red", "black")) +
    labs(color="Model")
)
```

We see that the Bayesian model is struggling. 

