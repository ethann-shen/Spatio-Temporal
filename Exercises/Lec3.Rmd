---
title: "Lec3"
author: "Ethan Shen"
date: "5/14/2020"
output: 
  html_document: 
    keep_md: yes
---

```{r}
library(tidyverse)
```

# GLMs

## Poisson Regression

Biggest issue: mean = variance

```{r}
aids = data_frame(
  year = 1981:1993,
  cases = c(12, 14, 33, 50, 67, 74, 123, 141, 165, 204, 253, 246, 240) %>% as.integer()
)

g = glm(cases~year, data=aids, family=poisson)


dev_resid = function(obs,pred) {
  sign(obs-pred) * sqrt(2*(obs*log(obs/pred)-(obs-pred)))
}

aids = aids %>% 
  mutate(pred = predict(g, newdata=., type = "response")) %>%
  mutate(standard = cases - pred, # standard residual 
         pearson = (cases - pred)/sqrt(pred), # Pearson residual 
         deviance = dev_resid(cases, pred)) # dviance 

aids %>%
  tidyr::gather(type, residual, -year, -cases, -pred) %>%
  mutate(type = forcats::as_factor(type)) %>%
  ggplot(aes(x=year, y=residual, color=type)) +
  geom_point() + 
  geom_segment(aes(xend=year, yend=0)) +
  labs(title = "Residual vs. Year") + 
  facet_wrap(~type, scale="free_y") + 
  guides(color=FALSE)

aids %>%
  tidyr::gather(type, residual, -year, -cases, -pred) %>%
  mutate(type = forcats::as_factor(type)) %>%
  ggplot(aes(x=pred, y=residual, color=type)) +
  geom_point() + 
  geom_segment(aes(xend=pred, yend=0)) +
  labs(title = "Residual vs. Predicted") + 
  facet_wrap(~type, scale="free_y") + 
  guides(color=FALSE)
```

## Updating the Model 

```{r}
g2 = glm(cases~poly(year, 2), data=aids, family=poisson)

aids = aids %>%
  mutate(pred2 = predict(g2, newdata=., type = "response")) %>%
  mutate(
    standard2 = cases - pred2,
    pearson2  = (cases - pred2)/sqrt(cases),
    deviance2 = dev_resid(cases, pred2)
  )

# in quadratic model, residuals are smaller, less structure in residuals 
aids %>%
  tidyr::gather(type, residual, -year, -cases, -pred, -pred2) %>%
  mutate(type = forcats::as_factor(type)) %>%
  ggplot(aes(x=year, y=residual, color=type)) +
  geom_point() + geom_segment(aes(xend=year, yend=0)) +
  facet_wrap(~type, scale="free_y") + 
  guides(color=FALSE)
```

## Bayesian Model 

```{r}
poisson_model = 
  "model{
  # Likelihood
  for (i in 1:length(Y)) {
    Y[i] ~ dpois(lambda[i]) # Poisson
    log(lambda[i]) <- beta[1] + beta[2]*X[i] # log link
    
    # In-sample prediction
    Y_hat[i] ~ dpois(lambda[i])
  }
  
  # prior for beta
  for (j in 1:2) {
    beta[j] ~ dnorm(0, 1/100)
  }
}"

n_burn=1000; n_iter=5000

m = rjags::jags.model(
  textConnection(poisson_model), quiet = TRUE,
  data = list(Y=aids$cases, X=aids$year)
)

update(m, n.iter=1000, progress.bar="none")

samp = rjags::coda.samples(
  m, variable.names=c("beta","lambda","Y_hat","Y","X"), 
  n.iter=5000, progress.bar="none"
)
```

### Model Fit 

```{r}
# terrible 
tidybayes::spread_draws(samp, Y_hat[i], lambda[i], X[i],Y[i]) %>%
  ungroup() %>%
  tidyr::gather(param, value, Y_hat, lambda) %>%
  ggplot(aes(x=X,y=Y)) +
  tidybayes::stat_lineribbon(aes(y=value), alpha=0.5) +
  geom_point() + 
  facet_wrap(~param)
```

### MCMC Diagnostics 

```{r}
# neither one is converging, seems like they are dependent
tidybayes::gather_draws(samp, beta[i]) %>%
  mutate(param = paste0(.variable,"[",i,"]")) %>%
  ggplot(aes(x=.iteration, y=.value)) + 
  geom_line() + 
  facet_wrap(~param, ncol=1, scale="free_y")
```

Model fit is bad, MCMC chain is not well-behaving. 

This is bcause the prior is uninformative. In the model, the intercept is -397 but the coefficient for `year` is 0.2021. 

Our prior is $N(0,10)$, which means values of $\beta$ will be between -30 and 30. This works for the `year` coefficient but NOT the intercept. Will take chain forever to explore the space for the intercept. 

## Fixing Model 

```{r}
# scaling data
summary(glm(cases~I(year-1981), data=aids, family=poisson))$coefficients
```

### Revising JAGS

```{r}
poisson_model_revised = 
  "model{
  # Likelihood
  for (i in 1:length(Y)) {
    Y[i] ~ dpois(lambda[i]) # Poisson
    log(lambda[i]) <- beta[1] + beta[2]*(X[i] - 1981) # mean-centering data
    
    # In-sample prediction
    Y_hat[i] ~ dpois(lambda[i])
  }
  
  # prior for beta
  for (j in 1:2) {
    beta[j] ~ dnorm(0, 1/100)
  }
}"

m = rjags::jags.model(
  textConnection(poisson_model_revised), quiet = TRUE,
  data = list(Y=aids$cases, X=aids$year)
)

update(m, n.iter=1000, progress.bar="none")

samp2 = rjags::coda.samples(
  m, variable.names=c("beta","lambda","Y_hat","Y","X"), 
  n.iter=5000, progress.bar="none"
)
```

```{r}
#y_hat is posterior predictive
tidybayes::spread_draws(samp2, Y_hat[i], lambda[i], X[i],Y[i]) %>%
  ungroup() %>%
  tidyr::gather(param, value, Y_hat, lambda) %>%
  ggplot(aes(x=X,y=Y)) +
  tidybayes::stat_lineribbon(aes(y=value), alpha=0.5) +
  geom_point() + 
  facet_wrap(~param)
```

```{r}
tidybayes::gather_draws(samp2, beta[i]) %>%
  mutate(param = paste0(.variable,"[",i,"]")) %>%
  ggplot(aes(x=.iteration, y=.value)) + 
  geom_line() + 
  facet_wrap(~param, ncol=1, scale="free_y")
```

Model fit is better & MCMC chain is well-behaving. 

However, there are almost 50% of points outside my 95% credible interval. Empirical coverage for 95% intereval is way off. Example of over-dispersion: more variability in the model than a Poisson model can handle. 

Quadratic model could fix this. 

```{r}
tmp = aids %>% select(X=year, standard:deviance) %>%
  tidyr::gather(resid, value, standard:deviance) %>%
  mutate(resid = forcats::as_factor(resid))


tidybayes::spread_samples(samp2, lambda[i], X[i], Y[i]) %>%
  ungroup() %>%
  mutate(
    standard = (Y-lambda),
    pearson  = (Y-lambda) / sqrt(lambda),
    deviance = dev_resid(Y, lambda)
  ) %>%
  tidyr::gather(resid, value, standard:deviance) %>%
  mutate(resid = forcats::as_factor(resid)) %>%
  ggplot(aes(x=as.factor(X),y=value,color=resid)) +
  geom_boxplot(outlier.alpha = 0.1) +
  geom_point(data=tmp, color="black") +
  facet_wrap(~resid, scale="free_y")
```

